- Class: meta
  Course: swirl
  Lesson: Unsupervised learning
  Author: Swirl Coders
  Type: Standard
  Organization: Johns Hopkins Bloomberg School of Public Health
  Version: 2.2.0

- Class: text
  Output: In this lesson we will cover two popular techniques for exploring and making sense of data,
    Hierarchical Clustering and K-means Clustering. These two clustering methods are part of a branch
    of statistical learning approaches known as 'Unsupervised Learning'. They are unsupervised in the
    sense that there is no predetermined organization of the data.

- Class: text
  Output:  First we'll learn about hierarchical clustering, a way of quickly examining and displaying
    multi-dimensional data. This technique is usually most useful in the early stages of analysis when
    you're trying to get an understanding of the data, e.g., finding some pattern or relationship between
    different factors or variables. As the name suggests hierarchical clustering creates a hierarchy of clusters.

- Class: text
  Output:  Clustering organizes data points that are close into groups. So obvious questions are
    "How do we define close?", "How do we group things?", and "How do we interpret the grouping?"
    Cluster analysis is a very important topic in data analysis.

- Class: figure
  Output: To give you an idea of what we're talking about, consider these random points we generated.
    We'll use them to demonstrate hierarchical clustering in this lesson. We'll do this in several steps,
    but first we have to clarify our terms and concepts.
  Figure: ranPoints.R
  FigureType: new

- Class: text
  Output: Hierarchical clustering is an agglomerative, or bottom-up, approach. From Wikipedia
    (http://en.wikipedia.org/wiki/Hierarchical_clustering), we learn that in this method,
    "each observation starts in its own cluster, and pairs of clusters are merged as one moves up
    the hierarchy." This means that we'll find the closest two points and put them together in one
    cluster, then find the next closest pair in the updated picture, and so forth. We'll repeat
    this process until we reach a reasonable stopping place.

- Class: text
  Output: Note the word "reasonable". There's a lot of flexibility in this field and how you
    perform your analysis depends on your problem. Again, Wikipedia tells us, "one can decide
    to stop clustering either when the clusters are too far apart to be merged (distance criterion)
    or when there is a sufficiently small number of clusters (number criterion)."

- Class: text
  Output: First, how do we define close? This is the most important step and there are several
    possibilities depending on the questions you're trying to answer and the data you have.
    Distance or similarity are usually the metrics used.

- Class: mult_question
  Output: In the given plot which pair points would you first cluster? Use distance as the metric.
  AnswerChoices:  7 and 8; 1 and 4; 5 and 6; 10 and 12
  CorrectAnswer:  5 and 6
  AnswerTests: omnitest(correctVal='5 and 6')
  Hint: The choices aren't very close. Look at the picture for the answer.

- Class: text
  Output: It's pretty obvious that out of the 4 choices, the pair 5 and 6 were the closest
    together. However, there are several ways to measure distance or similarity. Euclidean
    distance and correlation similarity are continuous measures, while Manhattan distance is
    a binary measure. In this lesson we'll just briefly discuss the first and last of these.
    It's important that you use a measure of distance that fits your problem.

- Class: figure
  Output: Euclidean distance is what you learned about in high school algebra. Given two
    points on a plane, (x1,y1) and (x2,y2), the Euclidean distance is the square root of the
    sums of the squares of the distances between the two x-coordinates (x1-x2) and the two
    y-coordinates (y1-y2). You probably recognize this as an application of the Pythagorean
    theorem which yields the length of the hypotenuse of a right triangle.
  Figure: showEuclid.R
  FigureType: new

- Class: text
  Output: It shouldn't be hard to believe that this generalizes to more than two dimensions
    as shown in the formula at the bottom of the picture shown here.

- Class: text
  Output: Euclidean distance is distance "as the crow flies". Many applications, however,
    can't realistically use crow-flying distance. Cars, for instance, have to follow roads.

- Class: figure
  Output:  In this case, we can use Manhattan or city block distance (also known as a taxicab metric).
    This picture, copied from http://en.wikipedia.org/wiki/Taxicab_geometry, shows what this means.
  Figure: showTaxi.R
  FigureType: new

- Class: text
  Output: You want to travel from the point at the lower left to the one on the top right.
    The shortest distance is the Euclidean (the green line), but you're limited to the grid,
    so you have to follow a path similar to those shown in red, blue, or yellow. These all have
    the same length (12) which is the number of small gray segments covered by their paths.

- Class: text
  Output: More formally, Manhattan distance is the sum of the absolute values of the distances
    between each coordinate, so the distance between the points (x1,y1) and (x2,y2) is |x1-x2|+|y1-y2|.
    As with Euclidean distance, this too generalizes to more than 2 dimensions.

- Class: figure
  Output:  Now we'll go back to our random points. You might have noticed that these points
    don't really look randomly positioned, and in fact, they're not. They were actually generated
    as 3 distinct clusters. We've put the coordinates of these points in a data frame for you,
    called 'dataFrame'.
  Figure: ranPoints.R
  FigureType: new

- Class: text
  Output:  We'll use this 'dataFrame' to demonstrate an agglomerative (bottom-up) technique of
    hierarchical clustering and create a dendrogram. This is an abstract picture (or graph) which
    shows how the 12 points in our dataset cluster together. Two clusters (initially, these are points)
    that are close are connected with a line, We'll use Euclidean distance as our metric of closeness.

- Class: cmd_question
  Output:  Run the R command 'dist()' with the argument 'dataFrame' to compute the distances between
    all pairs of these points. By default 'dist()' uses Euclidean distance as its metric, but other
    metrics such as Manhattan, are available. Just use the default.
  CorrectAnswer: dist(dataFrame)
  AnswerTests: omnitest(correctExpr='dist(dataFrame)')
  Hint: Type 'dist(dataFrame)' at the command prompt.

- Class: text
  Output: You see that the output is a lower triangular matrix with rows numbered from 2 to 12
    and columns numbered from 1 to 11. Entry (i,j) indicates the distance between points i and j.
    Clearly you need only a lower triangular matrix since the distance between points i and j
    equals that between j and i.

- Class: mult_question
  Output: From the output of 'dist()', what is the minimum distance between two points?
  AnswerChoices:  0.0815; 0.08317; -0.0700; 0.1085
  CorrectAnswer:   0.0815
  AnswerTests: omnitest(correctVal='0.0815')
  Hint: Recall a previous question where points 5 and 6 looked close.

- Class: figure
  Output: So 0.0815 (units are unspecified) between points 5 and 6 is the shortest distance.
    We can put these points in a single cluster and look for another close pair of points.
  Figure: cluster56.R
  FigureType: new

- Class: mult_question
  Output: Looking at the picture, what would be another good pair of points to put in another
    cluster given that 5 and 6 are already clustered?
  AnswerChoices:  7 and the cluster containing 5 ad 6; 10 and 11; 1 and 4; 7 and 8
  CorrectAnswer:   10 and 11
  AnswerTests: omnitest(correctVal='10 and 11')
  Hint: Which of the choices looks closest on the picture?

- Class: figure
  Output: So 10 and 11 are another pair of points that would be in a second cluster. We'll
    start creating our dendrogram now. Here's the original plot and two beginning pieces of
    the dendrogram.
  Figure: startDendro.R
  FigureType: new

- Class: cmd_question
  Output:  We can keep going like this in the obvious way and pair up individual points,
    but as luck would have it, R provides a simple function which you can call which creates
    a dendrogram for you. It's called 'hclust()' and takes as an argument the pairwise distance
    matrix which we looked at before. We've stored this matrix for you in a variable called
    'distxy'. Run 'hclust()' now with 'distxy' as its argument and assign the result to the variable 'hc'.
  CorrectAnswer: hc <- hclust(distxy)
  AnswerTests: expr_creates_var("hc"); omnitest(correctExpr='hc <- hclust(distxy)')
  Hint: Type 'hc <- hclust(distxy)' at the command prompt.

- Class: figure
  Output: You're probably curious and want to see hc.
  Figure: clearPlot.R
  FigureType: new

- Class: cmd_question
  Output: Call the R function 'plot()' with one argument, 'hc'.
  CorrectAnswer: plot(hc)
  AnswerTests: omnitest(correctExpr='plot(hc)')
  Hint: Type 'plot(hc)' at the command prompt.

- Class: cmd_question
  Output:  Nice plot, right? R's plot conveniently labeled everything for you. The points
    we saw are the leaves at the bottom of the graph, 5 and 6 are connected, as are 10 and 11.
    Moreover, we see that the original 3 groupings of points are closest together as leaves
    on the picture. That's reassuring.  Now call 'plot()' again, this time with the argument 'as.dendrogram(hc)'.
  CorrectAnswer: plot(as.dendrogram(hc))
  AnswerTests: omnitest(correctExpr='plot(as.dendrogram(hc))')
  Hint: Type 'plot(as.dendrogram(hc))' at the command prompt.

- Class: cmd_question
  Output: The essentials are the same, but the labels are missing and the leaves (original points)
    are all printed at the same level. Notice that the vertical heights of the lines and labeling
    of the scale on the left edge give some indication of distance. Use the R command 'abline()' to
    draw a horizontal blue line at 1.5 on this plot. Recall that this requires 2 arguments, 'h=1.5'
    and 'col="blue"'.
  CorrectAnswer: abline(h=1.5,col="blue")
  AnswerTests: omnitest(correctExpr='abline(h=1.5,col="blue")')
  Hint: Type 'abline(h=1.5,col="blue")' at the command prompt.

- Class: cmd_question
  Output: We see that this blue line intersects 3 vertical lines and this tells us that using
    the distance 1.5 (unspecified units) gives us 3 clusters (1 through 4), (9 through 12),
    and (5 through 8). We call this a "cut" of our dendrogram. Now cut the dendrogam by drawing
    a red horizontal line at .4.
  CorrectAnswer: abline(h=.4,col="red")
  AnswerTests: omnitest(correctExpr='abline(h=.4,col="red")')
  Hint: Type 'abline(h=.4,col="red")' at the command prompt.

- Class: cmd_question
  Output: How many clusters are there with a cut at this distance?
  CorrectAnswer: 5
  AnswerTests: equiv_val(5)
  Hint: How many vertical lines does this red line cross?

- Class: cmd_question
  Output: We see that by cutting at .4 we have 5 clusters, indicating that this distance is
    small enough to break up our original grouping of points. If we drew a horizontal line
    at .05, how many clusters would we get
  CorrectAnswer: 12
  AnswerTests: equiv_val(12)
  Hint: Recall that our shortest distance was around .08, so a distance smaller than that
    would make all the points their own private clusters.

- Class: cmd_question
  Output: Try it now (draw a horizontal line at .05) and make the line green.
  CorrectAnswer: abline(h=.05,col="green")
  AnswerTests: abline(h=.05,col="green")
  Hint: Type abline(h=.05,col="green") at the command prompt.

- Class: text
  Output: So the number of clusters in your data depends on where you draw the line!
    (We said there's a lot of flexibility here.) Now that we've seen the practice, let's go
    back to some "theory". Notice that the two original groupings, 5 through 8, and 9
    through 12, are connected with a horizontal line near the top of the display. You're
    probably wondering how distances between clusters of points are measured.

- Class: text
  Output: There are several ways to do this. We'll just mention two. The first is called
    complete linkage and it says that if you're trying to measure a distance between two
    clusters, take the greatest distance between the pairs of points in those two clusters.
    Obviously such pairs contain one point from each cluster.

- Class: figure
  Output: So if we were measuring the distance between the two clusters of points (1 through 4)
    and (5 through 8), using complete linkage as the metric, we would use the distance between
    points 4 and 8 as the measure since this is the largest distance between the pairs of those
    groups.
  Figure: complete.R
  FigureType: new

- Class: figure
  Output: The distance between the two clusters of points (9 through 12) and (5 through 8),
    using complete linkage as the metric, is the distance between points 11 and 8 since this
    is the largest distance between the pairs of those groups.
  Figure: complete2.R
  FigureType: new

- Class: figure
  Output: As luck would have it, the distance between the two clusters of points (9 through 12)
    and (1 through 4), using complete linkage as the metric, is the distance between points 11 and 4.
  Figure: complete3.R
  FigureType: new

- Class: cmd_question
  Output:  We've created the dataframe 'dFsm' for you containing these 3 points, 4, 8, and 11.
    Run 'dist()' on 'dFsm' to see what the smallest distance between these 3 points is.
  CorrectAnswer: dist(dFsm)
  AnswerTests: omnitest(correctExpr='dist(dFsm)')
  Hint: Type 'dist(dFsm)' at the command prompt.

- Class: figure
  Output: We see that the smallest distance is between points 2 and 3 in this reduced set,
    (these are actually points 8 and 11 in the original set), indicating that the two clusters
    these points represent ((5 through 8) and (9 through 12) respectively) would be joined
    (at a distance of 1.869) before being connected with the third cluster (1 through 4).
    This is consistent with the dendrogram we plotted.
  Figure: dendro.R
  FigureType: new

- Class: figure
  Output: The second way to measure a distance between two clusters that we'll just mention
    is called average linkage. First you compute an "average" point in each cluster (think of
    it as the cluster's center of gravity). You do this by computing the mean (average) x and
    y coordinates of the points in the cluster.
  Figure: average.R
  FigureType: new

- Class: figure
  Output: Then you compute the distances between each cluster average to compute the intercluster distance.
  Figure: average2.R
  FigureType: new

- Class: cmd_question
  Output:  Now look at the hierarchical cluster we created before, 'hc', by entering this variable in the
    console now.
  CorrectAnswer: hc
  AnswerTests: omnitest(correctExpr='hc')
  Hint: Type 'hc' at the command prompt.

- Class: mult_question
  Output: Which type of linkage did 'hclust()' use to agglomerate clusters?
  AnswerChoices:  average; complete
  CorrectAnswer:  complete
  AnswerTests: omnitest(correctVal='complete')
  Hint: Look at the output when you looked at hc. What was the cluster method?

- Class: text
  Output: In our simple set of data, the average and complete linkages aren't that different,
    but in more complicated datasets the type of linkage  you use could affect how your data
    clusters. It is a good idea to experiment with different methods of linkage to see the
    varying ways your data groups. This will help you determine the best way to continue with
    your analysis.

- Class: text
  Output: Let's review now.

- Class: mult_question
  Output:  What is the purpose of hierarchical clustering?
  AnswerChoices:  Give an idea of the relationships between variables or observations; Present a finished picture; Inspire other researchers; None of the others
  CorrectAnswer: Give an idea of the relationships between variables or observations
  AnswerTests: omnitest(correctVal='Give an idea of the relationships between variables or observations')
  Hint: Recall that this is a technique for EXPLORING data when you're first starting to research a problem.

- Class: mult_question
  Output:  True or False? When you're doing hierarchical clustering there are strict rules
    that you MUST follow.
  AnswerChoices:  True; False
  CorrectAnswer: False
  AnswerTests: omnitest(correctVal='False')
  Hint: There are always choices to be made in this area.

- Class: mult_question
  Output:  True or False? There's only one way to measure distance.
  AnswerChoices:  True; False
  CorrectAnswer: False
  AnswerTests: omnitest(correctVal='False')
  Hint: There are always choices to be made in this area.

- Class: mult_question
  Output:  True or False? Complete linkage is a method of computing distances between clusters.
  AnswerChoices:  True; False
  CorrectAnswer: True
  AnswerTests: omnitest(correctVal='True')
  Hint: Once a cluster contains more than one point you need a method of defining a distance
    between it and other clusters.

- Class: mult_question
  Output:  True or False? Average linkage uses the maximum distance between points of two
    clusters as the distance between those clusters.
  AnswerChoices:  True; False
  CorrectAnswer: False
  AnswerTests: omnitest(correctVal='False')
  Hint: Average linkage uses the average or mean point as the representative of a cluster.
    The distance between these average points is the distance between the clusters.

- Class: mult_question
  Output:  True or False? The number of clusters you derive from your data depends on the
    distance at which you choose to cut it.
  AnswerChoices:  True; False
  CorrectAnswer: True
  AnswerTests: omnitest(correctVal='True')
  Hint: Recall our example where we drew horizontal cut lines through the dendrogram.

- Class: text
  Output: Great! Let's move on to a second clustering method, k-means clustering.

- Class: text
  Output:  k-means clustering is another way of examining and organizing multi-dimensional
    data. As with hierarchical clustering, this technique is  most useful in the early stages
    of analysis when you're trying to get an understanding of the data, e.g., finding some
    pattern or relationship between different factors or variables.

- Class: text
  Output:  R documentation tells us that the k-means method "aims to partition the points
    into k groups such that the sum of squares from points to the assigned cluster centres is
    minimized."

- Class: text
  Output: Since clustering organizes data points that are close into groups we'll assume
    we've decided on a measure of distance, e.g., Euclidean.

- Class: figure
  Output: To illustrate the method, we'll use these random points we generated, familiar
    to you if you've already gone through the hierarchical clustering lesson. We'll  demonstrate
    k-means clustering in several steps, but first we'll explain the general idea.
  Figure: ranPoints.R
  FigureType: new

- Class: text
  Output: As we said, k-means is a partioning approach which requires that you first guess
    how many clusters you have (or want). Once you fix this number, you randomly create a "centroid"
    (a phantom point) for each cluster and assign each point or observation in your dataset
    to the centroid to which it is closest. Once each point is assigned a centroid, you readjust
    the centroid's position by making it the average of the points assigned to it.

- Class: text
  Output: Once you have repositioned the centroids, you must recalculate the distance of the
    observations to the centroids and reassign any, if necessary, to the centroid closest to
    them. Again, once the reassignments are done, readjust the positions of the centroids based
    on the new cluster membership. The process stops once you reach an iteration in which no
    adjustments are made or when you've reached some predetermined maximum number of iterations.

- Class: mult_question
  Output: As described, what does this process require?
  AnswerChoices:  A defined distance metric; A number of clusters; An initial guess as to cluster centroids; All of the others
  CorrectAnswer:  All of the others
  AnswerTests: omnitest(correctVal='All of the others')
  Hint: Which choice includes all the others.

- Class: mult_question
  Output: So k-means clustering requires some distance metric (say Euclidean), a hypothesized
    fixed number of clusters, and an initial guess as to cluster centroids. As described, what
    does this process produce?
  AnswerChoices:  A final estimate of cluster centroids; An assignment of each point to a cluster; All of the others
  CorrectAnswer:  All of the others
  AnswerTests: omnitest(correctVal='All of the others')
  Hint: Which choice includes all the others.

- Class: text
  Output: When it's finished k-means clustering returns a final position of each cluster's centroid
    as well as the assignment of each data point or observation to a cluster.

- Class: text
  Output: Now we'll step through this process using our random points as our data. The coordinates
    of these are stored in 2 vectors, 'x' and 'y'. We eyeball the display and guess that there are 3 clusters.
    We'll pick 3 positions of centroids, one for each cluster.

- Class: cmd_question
  Output:  We've created two 3-long vectors for you, 'cx' and 'cy'. These respectively hold the x- and y- coordinates
    for 3 proposed centroids. For convenience, we've also stored them in a 2 by 3 matrix 'cmat'.
    The x coordinates are in the first row and the y coordinates in the second. Look at 'cmat' now.
  CorrectAnswer: cmat
  AnswerTests: omnitest(correctExpr='cmat')
  Hint: Type cmat at the command prompt.

- Class: cmd_question
  Output:  The coordinates of these points are (1,2), (1.8,1.0) and (2.5,1.5). We'll add these centroids to
    the plot of our points. Do this by calling the R command 'points()' with 6 arguments. The first 2 are 'cx' and 'cy',
    and the third is 'col=' set equal to the concatenation ('c()') of 3 colors, "red", "orange", and "purple". The fourth
    argument is 'pch=' set equal to 3 (a plus sign), the fifth is 'cex=' set equal to 2 (expansion of character),
    and the final is 'lwd=' (line width) also set equal to 2.
  CorrectAnswer: points(cx,cy,col=c("red","orange","purple"),pch=3,cex=2,lwd=2)
  AnswerTests: omnitest(correctExpr='points(cx,cy,col=c("red","orange","purple"),pch=3,cex=2,lwd=2)')
  Hint: Type 'points(cx,cy,col=c("red","orange","purple"),pch=3,cex=2,lwd=2)' at the command prompt.

- Class: text
  Output: We see the first centroid (1,2) is in red. The second (1.8,1.0), to the right and below the first,
    is orange, and the final centroid (2.5,1.5), the furthest to the right, is purple.

- Class: mult_question
  Output: Now we have to calculate distances between each point and every centroid. There are 12 data points
    and 3 centroids. How many distances do we have to calculate?
  AnswerChoices:  15; 36; 9; 108
  CorrectAnswer: 36
  AnswerTests: omnitest(correctVal='36')
  Hint: The distance between each point and one centroid means 12 distances have to be calculated
    for each centroid. This has to be done for all 3 centroids.

- Class: cmd_question
  Output:  We've written a function for you called 'mdist()' which takes 4 arguments. The vectors
    of data points ('x' and 'y') are the first two and the two vectors of centroid coordinates
    ('cx' and 'cy') are the last two. Call 'mdist()' now with these arguments.
  CorrectAnswer: mdist(x,y,cx,cy)
  AnswerTests: omnitest(correctExpr='mdist(x,y,cx,cy)')
  Hint: Type 'mdist(x,y,cx,cy)' at the command prompt.

- Class: mult_question
  Output: We've stored these distances in the matrix 'distTmp' for you. Now we have to assign a
    cluster to each point. To do that we'll look at each column and ...?
  AnswerChoices:  pick the minimum entry; pick the maximum entry; add up the 3 entries.
  CorrectAnswer:  pick the minimum entry
  AnswerTests: omnitest(correctVal='pick the minimum entry')
  Hint: We assign each point to the centroid closest to it. Recall that the matrix holds distances.

- Class: mult_question
  Output: From the 'distTmp' entries, which cluster would point 6 be assigned to?
  AnswerChoices:  1; 2; 3; none of the above
  CorrectAnswer:  3
  AnswerTests: omnitest(correctVal='3')
  Hint: Which row in column 6 has the lowest value?

- Class: cmd_question
  Output:  R has a handy function 'which.min()' which you can apply to ALL the columns of 'distTmp'
    with one call. Simply call the R function 'apply()' with 3 arguments. The first is 'distTmp',
    the second is 2 meaning the columns of 'distTmp', and the third is the function 'which.min'
    (without the '()'), the function you want to apply to the columns of 'distTmp'. Try this now.
  CorrectAnswer: apply(distTmp,2,which.min)
  AnswerTests: omnitest(correctExpr='apply(distTmp,2,which.min)')
  Hint: Type 'apply(distTmp,2,which.min)' at the command prompt.

- Class: text
  Output: You can see that you were right and the 6th entry is indeed 3 as you answered before.
    We see the first 3 entries were assigned to the second (orange) cluster and only 2 points (4 and 8)
    were assigned to the first (red) cluster.

- Class: cmd_question
  Output:  We've stored the vector of cluster colors ("red","orange","purple") in the vector 'cols1'
    for you and we've also stored the cluster assignments in the vector 'newClust'. Let's color the 12
    data points according to their assignments. Again, use the command 'points()' with 5 arguments.
    The first 2 are 'x' and 'y'. The third is 'pch' set to 19, the fourth is 'cex' set to 2, and the last,
    'col' is set to 'cols1[newClust]'.
  CorrectAnswer: points(x,y,pch=19,cex=2,col=cols1[newClust])
  AnswerTests: omnitest(correctExpr='points(x,y,pch=19,cex=2,col=cols1[newClust])')
  Hint: Type 'points(x,y,pch=19,cex=2,col=cols1[newClust])' at the command prompt.

- Class: text
  Output: Now we have to recalculate our centroids so they are the average (center of gravity)
    of the cluster of points assigned to them. We have to do the 'x' and 'y' coordinates separately.
    We'll do the 'x' coordinate first. Recall that the vectors 'x' and 'y' hold the respective coordinates
    of our 12 data points.

- Class: cmd_question
  Output: We can use the R function 'tapply()' which applies "a function over a ragged array".
    This means that every element of the array is assigned a factor and the function is applied
    to subsets of the vector (identified by the factor vector). This allows us to take advantage
    of the factor vector 'newClust' we calculated. Call 'tapply()' now with 3 arguments, 'x' (the data),
    'newClust' (the factor vector), and 'mean' (the function to apply).
  CorrectAnswer: tapply(x,newClust,mean)
  AnswerTests: omnitest(correctExpr='tapply(x,newClust,mean)')
  Hint: Type 'tapply(x,newClust,mean)' at the command prompt.

- Class: cmd_question
  Output: Repeat the call, except now apply it to the vector 'y' instead of 'x'.
  CorrectAnswer: tapply(y,newClust,mean)
  AnswerTests: omnitest(correctExpr='tapply(y,newClust,mean)')
  Hint: Type 'tapply(y,newClust,mean)' at the command prompt.

- Class: cmd_question
  Output: Now that we have new 'x' and new 'y' coordinates for the 3 centroids we can plot them.
    We've stored off the coordinates for you in variables 'newCx' and 'newCy'. Use the R command
    'points()' with these as the first 2 arguments. In addition, use the arguments 'col' set equal
    to 'cols1', 'pch' equal to 8, 'cex' equal to 2 and 'lwd' also equal to 2.
  CorrectAnswer: points(newCx,newCy,col=cols1,pch=8,cex=2,lwd=2)
  AnswerTests: omnitest(correctExpr='points(newCx,newCy,col=cols1,pch=8,cex=2,lwd=2)')
  Hint: Type 'points(newCx,newCy,col=cols1,pch=8,cex=2,lwd=2)' at the command prompt.

- Class: cmd_question
  Output: We see how the centroids have moved closer to their respective clusters. This is
    especially true of the second (orange) cluster. Now call the distance function 'mdist()' with
    the 4 arguments 'x', 'y', 'newCx', and 'newCy'. This will allow us to reassign the data points to
    new clusters if necessary.
  CorrectAnswer: mdist(x,y,newCx,newCy)
  AnswerTests: omnitest(correctExpr='mdist(x,y,newCx,newCy)')
  Hint: Type 'mdist(x,y,newCx,newCy)' at the command prompt.

- Class: mult_question
  Output: We've stored off this new matrix of distances in the matrix 'distTmp2' for you.
    Recall that the first cluster is red, the second orange and the third purple. Look closely
    at columns 4 and 7 of 'distTmp2'. What will happen to points 4 and 7?
  AnswerChoices:  Nothing; They will both change to cluster 2; They will both change clusters; They're the only points that won't change clusters
  CorrectAnswer:  They will both change clusters
  AnswerTests: omnitest(correctVal='They will both change clusters')
  Hint: Two of the choices are obviously wrong. That leaves two possibilities which are similar. Look carefully at the numbers in columns 4 and 7 to see where the minimum values are.

- Class: cmd_question
  Output: Now call 'apply()' with 3 arguments, 'distTmp2', '2', and 'which.min' to find the new cluster assignments for the points.
  CorrectAnswer:  apply(distTmp2,2,which.min)
  AnswerTests: omnitest(correctExpr='apply(distTmp2,2,which.min)')
  Hint: Type 'apply(distTmp2,2,which.min)' at the command prompt.

- Class: cmd_question
  Output: We've stored off the new cluster assignments in a vector of factors called 'newClust2'.
    Use the R function 'points()' to recolor the points with their new assignments. Again, there are 5
    arguments, 'x' and 'y' are first, followed by 'pch' set to 19, 'cex' to 2, and 'col' to 'cols1[newClust2]'.
  CorrectAnswer:  points(x,y,pch=19,cex=2,col=cols1[newClust2])
  AnswerTests: omnitest(correctExpr='points(x,y,pch=19,cex=2,col=cols1[newClust2])')
  Hint: Type 'points(x,y,pch=19,cex=2,col=cols1[newClust2])' at the command prompt.

- Class: text
  Output: Notice that points 4 and 7 both changed clusters, 4 moved from 1 to 2 (red to orange),
    and point 7 switched from 3 to 2 (purple to red).

- Class: cmd_question
  Output: Now use 'tapply()' to find the x coordinate of the new centroid. Recall there are 3
    arguments, 'x', 'newClust2', and 'mean'.
  CorrectAnswer:  tapply(x,newClust2,mean)
  AnswerTests: omnitest(correctExpr='tapply(x,newClust2,mean)')
  Hint: Type 'tapply(x,newClust2,mean)' at the command prompt.

- Class: cmd_question
  Output: Do the same to find the new y coordinate.
  CorrectAnswer:  tapply(y,newClust2,mean)
  AnswerTests: omnitest(correctExpr='tapply(y,newClust2,mean)')
  Hint: Type 'tapply(y,newClust2,mean)' at the command prompt.

- Class: cmd_question
  Output: We've stored off these coordinates for you in the variables 'finalCx' and 'finalCy'.
    Plot these new centroids using the points function with 6 arguments. The first 2 are 'finalCx'
    and 'finalCy'. The argument 'col' should equal 'cols1', 'pch' should equal 9, 'cex' 2 and 'lwd' 2.
  CorrectAnswer:  points(finalCx,finalCy,col=cols1,pch=9,cex=2,lwd=2)
  AnswerTests: omnitest(correctExpr='points(finalCx,finalCy,col=cols1,pch=9,cex=2,lwd=2)')
  Hint: Type 'points(finalCx,finalCy,col=cols1,pch=9,cex=2,lwd=2)' at the command prompt.

- Class: text
  Output: It should be obvious that if we continued this process points 5 through 8 would all
    turn red, while points 1 through 4 stay orange, and points 9 through 12 purple.

- Class: text
  Output: Now that you've gone through an example step by step, you'll be relieved to hear that
    R provides a command to do all this work for you. Unsurprisingly it's called 'kmeans()' and,
    although it has several parameters, we'll just mention four. These are x, (the numeric matrix of data),
    centers, iter.max, and nstart. The second of these (centers) can be either a number of clusters or a
    set of initial centroids. The third, iter.max, specifies the maximum number of iterations to go through,
    and nstart is the number of random starts you want to try if you specify centers as a number.

- Class: cmd_question
  Output: Call 'kmeans()' now with 2 arguments, 'dataFrame' (which holds the x and y coordinates of our 12 points)
    and 'centers' set equal to 3.
  CorrectAnswer:  kmeans(dataFrame,centers=3)
  AnswerTests: omnitest(correctExpr='kmeans(dataFrame,centers=3)')
  Hint: Type  'kmeans(dataFrame,centers=3)' at the command prompt.

- Class: cmd_question
  Output: The program returns the information that the data clustered into 3 clusters each
    of size 4. It also returns the coordinates of the 3 cluster means, a vector named cluster
    indicating how the 12 points were partitioned into the clusters, and the sum of squares within
    each cluster. It also shows all the available components returned by the function. We've stored
    off this data for you in a kmeans object called 'kmObj'. Look at 'kmObj$iter' to see how many
    iterations the algorithm went through.
  CorrectAnswer:  kmObj$iter
  AnswerTests: omnitest(correctExpr='kmObj$iter')
  Hint: Type 'kmObj$iter' at the command prompt.

- Class: cmd_question
  Output: Two iterations as we did before. We just want to emphasize how you can access the
    information available to you. Let's plot the data points color coded according to their
    cluster. This was stored in 'kmObj$cluster'. Run 'plot()' with 5 arguments. The data, 'x' and 'y',
    are the first two; the third, 'col' is set equal to 'kmObj$cluster', and the last two are 'pch'
    and 'cex'. The first of these should be set to 19 and the last to 2.
  CorrectAnswer:  plot(x,y,col=kmObj$cluster,pch=19,cex=2)
  AnswerTests: omnitest(correctExpr='plot(x,y,col=kmObj$cluster,pch=19,cex=2)')
  Hint: Type 'plot(x,y,col=kmObj$cluster,pch=19,cex=2)' at the command prompt.

- Class: cmd_question
  Output: Now add the centroids which are stored in 'kmObj$centers'. Use the 'points()' function
    with 5 arguments. The first two are  'kmObj$centers' and 'col=c("black","red","green")'. The
    last three, 'pch', 'cex', and 'lwd', should all equal 3.
  CorrectAnswer:  points(kmObj$centers,col=c("black","red","green"),pch=3,cex=3,lwd=3)
  AnswerTests: omnitest(correctExpr='points(kmObj$centers,col=c("black","red","green"),pch=3,cex=3,lwd=3)')
  Hint: Type 'points(kmObj$centers,col=c("black","red","green"),pch=3,cex=3,lwd=3)' at the command prompt.

- Class: text
  Output: Now for some fun! We want to show you how the output of the 'kmeans()' function is affected
    by its random start (when you just ask for a number of clusters). With random starts you might
    want to run the function several times to get an idea of the relationships between your observations.
    We'll call 'kmeans()' with the same data points (stored in dataFrame), but ask for 6 clusters instead of 3.

- Class: cmd_question
  Output: We'll plot our data points several times and each time we'll just change the argument
    'col' which will show us how the R function 'kmeans()' is clustering them. So, call 'plot()' now with 5 arguments.
    The first 2 are 'x' and 'y'. The third is 'col' set equal to the call 'kmeans(dataFrame,6)$cluster'.
    The last two ('pch' and 'cex') are set to 19 and 2 respectively.
  CorrectAnswer:  plot(x,y,col=kmeans(dataFrame,6)$cluster,pch=19,cex=2)
  AnswerTests: omnitest(correctExpr='plot(x,y,col=kmeans(dataFrame,6)$cluster,pch=19,cex=2)')
  Hint: Type 'plot(x,y,col=kmeans(dataFrame,6)$cluster,pch=19,cex=2)' at the command prompt.

- Class: cmd_question
  Output: See how the points cluster? Now recall your last command (use the up arrow in the command history)
    and rerun it.
  CorrectAnswer:  plot(x,y,col=kmeans(dataFrame,6)$cluster,pch=19,cex=2)
  AnswerTests: omnitest(correctExpr='plot(x,y,col=kmeans(dataFrame,6)$cluster,pch=19,cex=2)')
  Hint: Type 'plot(x,y,col=kmeans(dataFrame,6)$cluster,pch=19,cex=2)' at the command prompt.

- Class: cmd_question
  Output: See how the clustering has changed? As the Teletubbies would say, "Again! Again!"
  CorrectAnswer:  plot(x,y,col=kmeans(dataFrame,6)$cluster,pch=19,cex=2)
  AnswerTests: omnitest(correctExpr='plot(x,y,col=kmeans(dataFrame,6)$cluster,pch=19,cex=2)')
  Hint: Type 'plot(x,y,col=kmeans(dataFrame,6)$cluster,pch=19,cex=2)' at the command prompt.

- Class: text
  Output: So the clustering changes with different starts. Perhaps 6 is too many clusters? Let's review!

- Class: mult_question
  Output: True or False? K-means clustering requires you to specify a number of clusters before you begin.
  AnswerChoices:  True; False
  CorrectAnswer:  True
  AnswerTests: omnitest(correctVal='True')
  Hint: What did you provide when you called the R function?

- Class: mult_question
  Output:  True or False? K-means clustering requires you to specify a number of iterations before you begin.
  AnswerChoices:  True; False
  CorrectAnswer:  False
  AnswerTests: omnitest(correctVal='False')
  Hint: What did you provide when you called the R function?

- Class: mult_question
  Output:  True or False? Every data set has a single fixed number of clusters.
  AnswerChoices:  True; False
  CorrectAnswer:  False
  AnswerTests: omnitest(correctVal='False')
  Hint: The number of clusters depends on your eye.

- Class: mult_question
  Output:  True or False? K-means clustering will always stop in 3 iterations
  AnswerChoices:  True; False
  CorrectAnswer:  False
  AnswerTests: omnitest(correctVal='False')
  Hint: The number of iterations depends on your data.

- Class: mult_question
  Output:  True or False? When starting 'kmeans()' with random centroids, you'll always end
    up with the same final clustering.
  AnswerChoices:  True; False
  CorrectAnswer:  False
  AnswerTests: omnitest(correctVal='False')
  Hint: Recall the last experiment we did in the lesson, rerunning the same routine.

- Class: text
  Output: Congratulations! You've made it through this introduction to unsupervised learning.

